{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Migrated from https://github.com/RylanSchaeffer/KoyejoLab-Revisiting-Model-Collapse/blob/main/language_modeling_experiments/plot_matthias_cameraready.ipynb\n",
    "\n",
    "\n",
    "wandb_entity = \"mgerstgrasser\"\n",
    "group_name = \"tinystories_models_1\"\n",
    "\n",
    "run_groups_to_fetch = [\n",
    "    \"tinystories_models_1\",\n",
    "    # \"tinystories_1_matthias_8k_1epoch_llama\",\n",
    "    # \"tinystories_1_matthias_8k_1epoch_llama42M_snap\",\n",
    "    # \"tinystories_1_matthias_8k_1epoch_llama125M_nlp\",\n",
    "    # \"tinystories_5_matthias_8k_3epoch_new\",\n",
    "    # \"tinystories_5_matthias_8k_1epoch_new\",\n",
    "    # \"tinystories_1_matthias_8k_1epoch_subsample\",\n",
    "    # \"tinystories_llama_firstiter\",\n",
    "]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datetime import datetime\n",
    "import joblib\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "\n",
    "\n",
    "import src.plot"
   ],
   "id": "f4354b1bed6b2fed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We have the same vocab size everywhere, so same uniform loss.\n",
    "uniform_loss = math.log(8000)"
   ],
   "id": "5c1594c6ec2a9ea0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create API to retrieve the runs from wandb\n",
    "api = wandb.Api(timeout=100)"
   ],
   "id": "300ccac26ec9715a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get the current date and time\n",
    "# query_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# query_datetime = \"20240327_185710\"\n",
    "# query_datetime = \"20240329_220900\"\n",
    "query_datetime = \"20240809\"\n",
    "plot_dir = \"figures\"\n",
    "data_dir = \"data\"\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def load_data(group_name, query_datetime, data_dir: str = \"data\"):\n",
    "    print(f\"Loading data for group {group_name} at t= {query_datetime}.\")\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    cached_results_path = os.path.join(\n",
    "        data_dir, f\"cache_{group_name}_{query_datetime}.joblib\"\n",
    "    )\n",
    "    if os.path.exists(cached_results_path):\n",
    "        cached_results = joblib.load(cached_results_path)\n",
    "        # Load data from cache if the file exists\n",
    "        data_inner = cached_results[\"df_inner\"]\n",
    "        data_outer = cached_results[\"df_outer\"]\n",
    "        print(f\"Loaded data_inner and data_outer for group {group_name} from disk.\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"Data for group {group_name} at t= {query_datetime} not found. Fetching data from W&B API.\"\n",
    "        )\n",
    "\n",
    "        # Your existing code to fetch data from the API\n",
    "        runs = api.runs(\n",
    "            f\"{wandb_entity}/model_collapse_gpt\",\n",
    "            filters={\n",
    "                \"group\": group_name,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        data_inner = []\n",
    "        data_outer = []\n",
    "        model_num_parameters = None\n",
    "\n",
    "        # Iterate over the runs and extract the required data\n",
    "        for run in runs:\n",
    "            if \"dataset_mode\" in run.config and \"model_num_parameters\" in run.config:\n",
    "                dataset_mode = run.config[\"dataset_mode\"]\n",
    "                model_num_parameters = run.config[\"model_num_parameters\"]\n",
    "                model = run.config[\"model\"]\n",
    "                history = run.scan_history(page_size=100000)\n",
    "                run_config = {\n",
    "                    \"temperature\": run.config[\"full_cli_args\"][\n",
    "                        \"generation_temperature\"\n",
    "                    ],\n",
    "                    \"model\": model,\n",
    "                    \"model_num_parameters\": model_num_parameters,\n",
    "                    \"dataset_mode\": dataset_mode,\n",
    "                    \"run_group\": group_name,\n",
    "                    \"run_id\": run.id,\n",
    "                    \"dataset_fraction\": (\n",
    "                        run.config[\"dataset_fraction\"]\n",
    "                        if run.config[\"dataset_fraction\"] is not None\n",
    "                        else 1.0\n",
    "                    ),\n",
    "                    \"generation_strategy\": (\n",
    "                        run.config[\"full_cli_args\"][\"generation_strategy\"]\n",
    "                    ),\n",
    "                    \"num_epochs\": run.config[\"epochs\"],\n",
    "                    \"firstiter_steps\": run.config[\"full_cli_args\"].get(\n",
    "                        \"num_steps_first_iter\", -1\n",
    "                    ),\n",
    "                    \"hyperparameters\": f\"{model}_{model_num_parameters}_frac{run.config['dataset_fraction'] if run.config['dataset_fraction'] is not None else 1.0}_epochs{run.config['epochs']}_temp{run.config['full_cli_args']['generation_temperature']}\",\n",
    "                }\n",
    "                for row in history:\n",
    "                    try:\n",
    "                        if row[\"is_outer\"] is True:\n",
    "                            data_outer.append(\n",
    "                                {\n",
    "                                    \"step\": row[\"_step\"],\n",
    "                                    \"iteration\": row.get(\"iteration\", np.nan),\n",
    "                                    \"trainer_global_step\": row.get(\n",
    "                                        \"trainer_global_step\", np.nan\n",
    "                                    ),\n",
    "                                    \"eval_loss_outer\": row.get(\n",
    "                                        \"eval_loss_outer\", np.nan\n",
    "                                    ),\n",
    "                                    \"eval_perplexity_outer\": row.get(\n",
    "                                        \"eval_perplexity_outer\", np.nan\n",
    "                                    ),\n",
    "                                    **run_config,\n",
    "                                }\n",
    "                            )\n",
    "                        else:\n",
    "                            data_inner.append(\n",
    "                                {\n",
    "                                    \"step\": row[\"_step\"],\n",
    "                                    \"loss\": row.get(\"loss\", np.nan),\n",
    "                                    \"eval_loss\": row.get(\"eval_loss\", np.nan),\n",
    "                                    \"iteration\": row.get(\"iteration\", np.nan),\n",
    "                                    \"trainer_global_step\": row.get(\n",
    "                                        \"trainer_global_step\", np.nan\n",
    "                                    ),\n",
    "                                    \"epoch\": row[\"epoch\"],\n",
    "                                    **run_config,\n",
    "                                }\n",
    "                            )\n",
    "                    except KeyError as ke:\n",
    "                        print(f\"KeyError {ke} in run {run.id}, row {row}.\")\n",
    "\n",
    "        # df_inner = pd.DataFrame(data_inner)\n",
    "        # df_outer = pd.DataFrame(data_outer)\n",
    "\n",
    "        # Save the DataFrames to cache as CSV files\n",
    "        cached_results = {\n",
    "            \"df_inner\": data_inner,\n",
    "            \"df_outer\": data_outer,\n",
    "        }\n",
    "        joblib.dump(cached_results, filename=cached_results_path)\n",
    "\n",
    "        print(\n",
    "            f\"Data for group {group_name} at t=  {query_datetime} fetched from W&B API and written to disk.\"\n",
    "        )\n",
    "\n",
    "    return data_inner, data_outer\n",
    "\n",
    "\n",
    "# Load or fetch the data\n",
    "data_inner = []\n",
    "data_outer = []\n",
    "for group_name in run_groups_to_fetch:\n",
    "    data_inner_group, data_outer_group = load_data(\n",
    "        group_name, query_datetime, data_dir=data_dir\n",
    "    )\n",
    "    data_inner.extend(data_inner_group)\n",
    "    data_outer.extend(data_outer_group)\n",
    "\n",
    "df_inner = pd.DataFrame(data_inner)\n",
    "df_outer = pd.DataFrame(data_outer)\n",
    "\n",
    "\n",
    "# Skip this, not sure why I even ran this.\n",
    "df_inner = df_inner[df_inner[\"hyperparameters\"] != \"gpt2_8890880_frac5_epochs3_temp0.3\"]\n",
    "df_outer = df_outer[df_outer[\"hyperparameters\"] != \"gpt2_8890880_frac5_epochs3_temp0.3\"]\n",
    "\n",
    "# Skip subsample run with wrong batch size\n",
    "df_inner = df_inner[df_inner[\"run_id\"] != \"dty86yke\"]\n",
    "df_outer = df_outer[df_outer[\"run_id\"] != \"dty86yke\"]\n",
    "df_inner = df_inner[df_inner[\"run_id\"] != \"3ov1gya8\"]\n",
    "df_outer = df_outer[df_outer[\"run_id\"] != \"3ov1gya8\"]\n",
    "\n",
    "\n",
    "df_inner[\"dataset_mode\"] = df_inner[\"dataset_mode\"].map(\n",
    "    {\n",
    "        \"concatenate\": \"Accumulate\",\n",
    "        \"replace\": \"Replace\",\n",
    "        \"replace_multiple\": \"Replace (Multiple)\",\n",
    "        \"concatenate_subsample\": \"Accumulate-Subsample\",\n",
    "    }\n",
    ")\n",
    "\n",
    "df_outer[\"dataset_mode\"] = df_outer[\"dataset_mode\"].map(\n",
    "    {\n",
    "        \"concatenate\": \"Accumulate\",\n",
    "        \"replace\": \"Replace\",\n",
    "        \"replace_multiple\": \"Replace (Multiple)\",\n",
    "        \"concatenate_subsample\": \"Accumulate-Subsample\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# def construct_model_name(row):\n",
    "#     if row[\"model\"] == \"gpt2\":\n",
    "#         model_name_str = \"GPT-2-8M\"\n",
    "#     elif row[\"run_group\"] == \"tinystories_1_matthias_8k_1epoch_llama42M_snap\":\n",
    "#         model_name_str = \"Llama-2-42M\"\n",
    "#     elif row[\"run_group\"] == \"tinystories_1_matthias_8k_1epoch_llama125M_nlp\":\n",
    "#         model_name_str = \"Llama-2-125M\"\n",
    "#     elif row[\"run_group\"] == \"tinystories_1_matthias_8k_1epoch_llama\":\n",
    "#         model_name_str = \"Llama-2-12M\"\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unknown model: {row['model']}\")\n",
    "#     return f\"{model_name_str} ({row['model_num_parameters']/1e6:.0f}M)\"\n",
    "\n",
    "\n",
    "def construct_model_name(row):\n",
    "    if row[\"model\"] == \"gpt2\":\n",
    "        return \"GPT-2 (9M)\"\n",
    "    elif row[\"model\"] == \"llama2\":\n",
    "        return \"Llama-2 (12M)\"\n",
    "    elif row[\"model\"] == \"llama2-42M\":\n",
    "        return \"Llama-2 (42M)\"\n",
    "    elif row[\"model\"] == \"llama2-125M\":\n",
    "        return \"Llama-2 (126M)\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {row['model']}\")\n",
    "    return f\"{model_name_str} ({row['model_num_parameters']/1e6:.0f}M)\"\n",
    "\n",
    "\n",
    "df_inner[\"Model\"] = df_inner.apply(lambda row: construct_model_name(row=row), axis=1)\n",
    "df_outer[\"Model\"] = df_outer.apply(lambda row: construct_model_name(row=row), axis=1)\n",
    "\n",
    "# hyperparams_to_nice_model_name = {\n",
    "#     'gpt2_8890880_frac1.0_epochs1_temp0.3': 'GPT-2-9M (temp=0.3)',\n",
    "#     'gpt2_8890880_frac1.0_epochs1_temp1': 'GPT-2-9M',\n",
    "#     'llama2_12488960_frac1.0_epochs1_temp1': 'Llama-2-12M',\n",
    "#     'llama2_41755136_frac1.0_epochs1_temp1': 'Llama-2-42M',\n",
    "#     'llama2_125553408_frac1.0_epochs1_temp1': 'Llama-2-125M',\n",
    "#     'gpt2_8890880_frac5_epochs3_temp0.3': 'GPT-2-8M (temp=0.3, small dataset, 3 epochs)',\n",
    "#     'gpt2_8890880_frac5_epochs3_temp1': 'GPT-2-8M (small dataset, 3 epochs)',\n",
    "#     'gpt2_8890880_frac5_epochs1_temp1': 'GPT-2-8M (small dataset)'\n",
    "# }\n",
    "\n",
    "hyperparams_to_nice_model_name = {\n",
    "    \"gpt2_8890880_frac1.0_epochs1_temp0.3\": \" (temp=0.3)\",\n",
    "    \"gpt2_8890880_frac1.0_epochs1_temp1\": \"\",\n",
    "    \"llama2_12488960_frac1.0_epochs1_temp1\": \"\",\n",
    "    \"llama2_41755136_frac1.0_epochs1_temp1\": \"\",\n",
    "    \"llama2_125553408_frac1.0_epochs1_temp1\": \"\",\n",
    "    \"gpt2_8890880_frac5_epochs3_temp0.3\": \" (temp=0.3, small dataset, 3 epochs)\",\n",
    "    \"gpt2_8890880_frac5_epochs3_temp1\": \" (small dataset, 3 epochs)\",\n",
    "    \"gpt2_8890880_frac5_epochs1_temp1\": \" (small dataset)\",\n",
    "}\n",
    "\n",
    "\n",
    "def construct_model_name_long(row):\n",
    "    return row[\"Model\"]\n",
    "    if (\n",
    "        row[\"hyperparameters\"].startswith(\"gpt2_8890880_frac1.0_epochs1_temp\")\n",
    "        and not row[\"hyperparameters\"] in hyperparams_to_nice_model_name\n",
    "    ):\n",
    "        model_name_str = row[\"Model\"] + \"(\" + str(row[\"temperature\"]) + \")\"\n",
    "    elif row[\"hyperparameters\"] in hyperparams_to_nice_model_name:\n",
    "        model_name_str = (\n",
    "            row[\"Model\"] + hyperparams_to_nice_model_name[row[\"hyperparameters\"]]\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {row['model']}\")\n",
    "    return model_name_str\n",
    "\n",
    "\n",
    "df_inner[\"Model (long)\"] = df_inner.apply(\n",
    "    lambda row: construct_model_name_long(row=row), axis=1\n",
    ")\n",
    "df_outer[\"Model (long)\"] = df_outer.apply(\n",
    "    lambda row: construct_model_name_long(row=row), axis=1\n",
    ")"
   ],
   "id": "e7a4e40278044fca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_inner[\"run_id\"].unique()",
   "id": "99196f6697b39cbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_inner[\"Model (long)\"].unique()",
   "id": "1b4220db58a642bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_inner.head()",
   "id": "3de18bca976c655d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "col_order = [\"Replace\", \"Accumulate-Subsample\", \"Accumulate\"]",
   "id": "e284d8dc3f17efc1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Filter the DataFrame for trainer global step vs eval loss plot\n",
    "df_trainer_eval_loss = df_inner[\n",
    "    [\n",
    "        \"trainer_global_step\",\n",
    "        \"eval_loss\",\n",
    "        \"loss\",\n",
    "        \"iteration\",\n",
    "        \"dataset_mode\",\n",
    "        \"epoch\",\n",
    "        \"temperature\",\n",
    "        \"model\",\n",
    "        \"model_num_parameters\",\n",
    "        \"run_group\",\n",
    "        \"Model\",\n",
    "        \"Model (long)\",\n",
    "        \"generation_strategy\",\n",
    "    ]\n",
    "].dropna()\n",
    "\n",
    "df_trainer_eval_loss = df_trainer_eval_loss[df_trainer_eval_loss[\"temperature\"] == 1.0]\n",
    "\n",
    "df_trainer_eval_loss = df_trainer_eval_loss[\n",
    "    df_trainer_eval_loss[\"generation_strategy\"] == \"first_token_inclusive\"\n",
    "]\n",
    "\n",
    "df_trainer_eval_loss = df_trainer_eval_loss[df_trainer_eval_loss[\"iteration\"] < 5]\n",
    "\n",
    "\n",
    "# We use 1-based indexing in the mathematical notation.\n",
    "df_trainer_eval_loss[\"iteration\"] += 1\n",
    "\n",
    "\n",
    "# Rename column from \"iteration\" to \"Iteration\" for nicer plotting.\n",
    "df_trainer_eval_loss = df_trainer_eval_loss.rename(\n",
    "    columns={\"iteration\": \"Iteration\", \"temperature\": \"Temperature\"}\n",
    ")"
   ],
   "id": "8cce89d0feba6223"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_fig_1 = df_trainer_eval_loss\n",
    "df_fig_1 = df_fig_1.rename(columns={\"Model (long)\": \"Model\", \"Model\": \"Model (short)\"})\n",
    "df_fig_1 = df_fig_1[df_fig_1[\"Model\"] != \"Llama-2-12M\"]\n",
    "# Create a relplot for trainer global step vs eval loss\n",
    "g = sns.relplot(\n",
    "    # data=df_trainer_eval_loss[df_trainer_eval_loss[\"Temperature\"] == 0.3],\n",
    "    data=df_fig_1[df_fig_1[\"Temperature\"] == 1.0],\n",
    "    x=\"epoch\",\n",
    "    y=\"eval_loss\",\n",
    "    hue=\"Iteration\",\n",
    "    # style=\"Temperature\",\n",
    "    # style_order=[1.0, 0.3],\n",
    "    row=\"dataset_mode\",\n",
    "    col=\"Model\",\n",
    "    row_order=col_order,\n",
    "    kind=\"line\",\n",
    "    # palette=\"mako\",\n",
    "    palette=\"copper\",\n",
    "    # palette=\"crest\",\n",
    "    # palette=\"cool\",\n",
    "    # palette=\"flare\",\n",
    "    facet_kws={\"sharey\": True, \"sharex\": True, \"margin_titles\": True},\n",
    "    legend=\"full\",\n",
    ")\n",
    "g.set_axis_labels(\"Epoch\", \"Cross Entropy (Test)\")\n",
    "g.set_titles(\n",
    "    col_template=\"{col_name}\",\n",
    "    row_template=\"{row_name}\",\n",
    ")\n",
    "# g.map(\n",
    "#     plt.axhline, y=uniform_loss, color=\"red\", linestyle=\"--\", label=\"Uniform\"\n",
    "# )  # Add horizontal line\n",
    "sns.move_legend(g, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "g.fig.suptitle(f\"Language Models Pretrained on TinyStories\")\n",
    "src.plot.save_plot_with_multiple_extensions(\n",
    "    plot_dir=plot_dir,\n",
    "    plot_title=f\"eval_loss_linear_vs_epoch_linear_col_iteration_{query_datetime}\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ],
   "id": "5d12887442db72da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_fig_1 = df_trainer_eval_loss\n",
    "df_fig_1 = df_fig_1.rename(columns={\"Model (long)\": \"Model\", \"Model\": \"Model (short)\"})\n",
    "df_fig_1 = df_fig_1[df_fig_1[\"Model\"] != \"Llama-2-12M\"]\n",
    "\n",
    "for xaxis in [\"trainer_global_step\", \"epoch\"]:\n",
    "    for scale in [\"linear\", \"log\"]:\n",
    "        # Create a relplot for trainer global step vs eval loss\n",
    "        g = sns.relplot(\n",
    "            # data=df_trainer_eval_loss[df_trainer_eval_loss[\"Temperature\"] == 0.3],\n",
    "            data=df_fig_1[df_fig_1[\"Temperature\"] == 1.0],\n",
    "            x=xaxis,\n",
    "            y=\"eval_loss\",\n",
    "            hue=\"Iteration\",\n",
    "            # style=\"Temperature\",\n",
    "            # style_order=[1.0, 0.3],\n",
    "            col=\"dataset_mode\",\n",
    "            row=\"Model\",\n",
    "            col_order=col_order,\n",
    "            kind=\"line\",\n",
    "            # palette=\"mako\",\n",
    "            palette=\"copper\",\n",
    "            # palette=\"crest\",\n",
    "            # palette=\"cool\",\n",
    "            # palette=\"flare\",\n",
    "            facet_kws={\"sharey\": True, \"sharex\": True, \"margin_titles\": True},\n",
    "            legend=\"full\",\n",
    "        )\n",
    "        if scale == \"log\":\n",
    "            g.set(xscale=\"log\", yscale=\"log\")\n",
    "        if xaxis == \"epoch\":\n",
    "            g.set_axis_labels(\"Epoch\", \"Cross Entropy (Test)\")\n",
    "        else:\n",
    "            g.set_axis_labels(\"Gradient Step\", \"Cross Entropy (Test)\")\n",
    "        g.set_titles(\n",
    "            col_template=\"{col_name}\",\n",
    "            row_template=\"{row_name}\",\n",
    "        )\n",
    "        # g.map(\n",
    "        #     plt.axhline, y=uniform_loss, color=\"red\", linestyle=\"--\", label=\"Uniform\"\n",
    "        # )  # Add horizontal line\n",
    "        sns.move_legend(g, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "        g.fig.suptitle(f\"Language Models Pretrained on TinyStories\")\n",
    "        src.plot.save_plot_with_multiple_extensions(\n",
    "            plot_dir=plot_dir,\n",
    "            plot_title=f\"eval_loss_{scale}_vs_{xaxis}_linear_flipped_{query_datetime}\",\n",
    "        )\n",
    "\n",
    "        plt.show()"
   ],
   "id": "f0f7b79525892d70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cond = ((df_inner[\"epoch\"] == 1.0) & (df_inner[\"num_epochs\"] == 1.0)) | (\n",
    "    (df_inner[\"epoch\"] == 3.0) & (df_inner[\"num_epochs\"] == 3.0)\n",
    ")\n",
    "# df_fig1_alt = df_inner[cond]\n",
    "cond = df_trainer_eval_loss[\"epoch\"] == 1.0\n",
    "df_fig1_alt = df_trainer_eval_loss[cond]"
   ],
   "id": "7c76b96353dee168"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_fig1_alt = df_fig1_alt\n",
    "# df_fig1_alt = df_fig1_alt.rename(columns={\"Model (long)\": \"Model\", \"Model\": \"Model (short)\"})\n",
    "# df_fig1_alt = df_fig1_alt[df_fig1_alt[\"Model\"] != \"Llama-2-12M\"]\n",
    "\n",
    "# Create a relplot for trainer global step vs eval loss\n",
    "g = sns.relplot(\n",
    "    # data=df_trainer_eval_loss[df_trainer_eval_loss[\"Temperature\"] == 0.3],\n",
    "    data=df_fig1_alt[df_fig1_alt[\"Temperature\"] == 1.0]\n",
    "    .rename(columns={\"Model\": \"Model (short)\"})\n",
    "    .rename(\n",
    "        columns={\n",
    "            \"Model (long)\": \"Model\",\n",
    "        }\n",
    "    ),\n",
    "    x=\"Iteration\",\n",
    "    y=\"eval_loss\",\n",
    "    # hue=\"Iteration\",\n",
    "    # style=\"Temperature\",\n",
    "    # style_order=[1.0, 0.3],\n",
    "    col=\"dataset_mode\",\n",
    "    hue=\"Model\",\n",
    "    hue_order=[\n",
    "        \"GPT-2 (9M)\",\n",
    "        \"Llama-2 (12M)\",\n",
    "        \"Llama-2 (42M)\",\n",
    "        \"Llama-2 (126M)\",\n",
    "    ],\n",
    "    col_order=col_order,\n",
    "    kind=\"line\",\n",
    "    # palette=\"mako\",\n",
    "    # palette=\"copper\",\n",
    "    # palette=\"crest\",\n",
    "    # palette=\"cool\",\n",
    "    # palette=\"flare\",\n",
    "    facet_kws={\"sharey\": True, \"sharex\": True, \"margin_titles\": True},\n",
    "    legend=\"full\",\n",
    "    marker=\"o\",\n",
    "    markersize=10,\n",
    ")\n",
    "g.set_axis_labels(\"Model-Fitting Iteration\", \"Cross Entropy (Test)\")\n",
    "g.set_titles(\n",
    "    col_template=\"{col_name}\",\n",
    "    row_template=\"{row_name}\",\n",
    ")\n",
    "# g.set( yscale=\"log\")\n",
    "# g.map(\n",
    "#     plt.axhline, y=uniform_loss, color=\"red\", linestyle=\"--\", label=\"Uniform\"\n",
    "# )  # Add horizontal line\n",
    "# sns.move_legend(g, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "sns.move_legend(g, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "g.fig.suptitle(f\"Language Models Pretrained on TinyStories\")\n",
    "src.plot.save_plot_with_multiple_extensions(\n",
    "    plot_dir=plot_dir,\n",
    "    plot_title=f\"eval_loss_linear_vs_iteration_{query_datetime}\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ],
   "id": "bfb2ab7f2759b05b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
